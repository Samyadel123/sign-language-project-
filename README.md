# ðŸ–ï¸ Hand Gesture Recognition

## ðŸ”· Project Overview

**Project name:** Hand Gesture Recognition (CNN + MLflow)

**Goal:** Build a clear and reproducible pipeline to train a Convolutional Neural Network (CNN) that classifies static hand gestures (images). Use MLflow to track experiments and save the final model for delivery.

**Scope (initial):** Static images of hand gestures (alphabet/words). Dynamic gestures / sentence-level translation are out of scope for the first version.

---

## ðŸ§­ Full Pipeline (high-level)

1. **Data collection & organization** â€” gather images or use an existing dataset. Keep metadata (subject id, session id).
2. **Preprocessing** â€” resize, normalize, optional cropping, color handling.
3. **Data splitting** â€” split by subject/session (to avoid leakage), stratified by class â†’ train/val/test.
4. **Augmentation** â€” applied only to the *training* set.
5. **Model training** â€” baseline CNN; add regularization and tuning.
6. **Evaluation** â€” evaluate on hold-out test set; compute metrics and confusion matrix.
7. **Experiment tracking** â€” use MLflow: log params, metrics, artifacts.
8. **Save & package** â€” export model file(s) and environment description.
9. **Deliver & demo** â€” README, code, model, MLflow runs, short demo or instruction to run.

---

## ðŸ“ Data Folder Structure

```
project-root/
â”œâ”€ data/
â”‚  â”œâ”€ raw/
â”‚  â”‚  â”œâ”€ subject_01/
â”‚  â”‚  â””â”€ subject_02/
â”‚  â”œâ”€ processed/
â”‚  â”‚  â”œâ”€ train/
â”‚  â”‚  â”œâ”€ val/
â”‚  â”‚  â””â”€ test/
â”‚  â””â”€ metadata.csv
â”œâ”€ src/
â”‚  â”œâ”€ preprocess.py
â”‚  â”œâ”€ train.py
â”‚  â”œâ”€ evaluate.py
â”‚  â””â”€ mlflow_utils.py
â”œâ”€ notebooks/
â”œâ”€ models/
â”œâ”€ requirements.txt
â””â”€ README.md
```

---

## ðŸ§¹ Data & Preprocessing (detailed)

**1. Image format & color**

- Keep RGB images if color information is helpful; otherwise grayscale reduces compute.

**2. Resize**

- Choose a fixed resolution: **64Ã—64** or **128Ã—128**.
  - 64Ã—64: faster training, less memory, may lose fine details.
  - 128Ã—128: better for finer finger poses but slower.

**3. Normalization**

- Scale pixel values to `[0, 1]` (divide by 255). Optionally use per-channel mean/std normalization if using pretrained backbones.

**4. Augmentation (apply only to training set)**

- Rotation: Â±15Â°
- Width/height shift: up to 10% (0.1)
- Zoom range: 0.9â€“1.1
- Brightness variation: Â±20%
- Horizontal flip: use with caution â€” only if flips do not change label meaning (many sign gestures are asymmetric).
- Minor shear or random crop if needed.

**5. Extra preprocessing considerations**

- If dataset has bounding boxes or hand crops, center/align images so hand occupies a consistent region.
- Keep a reproducible script that performs preprocessing and writes processed images or a single `.npz`/`.npy` file.

---

## ðŸ§© Avoiding Data Leakage â€” Recommended Practice

- **Split by subject/session**: ensure each subject (person) appears in exactly one of train/val/test. This prevents the model from learning person-specific clues.
- **Stratify** by class when splitting, to keep class distribution consistent.
- **Augment after splitting**: do augmentation only on training set, never on validation/test.
- **Fix random seeds** for reproducibility and store the indices of each split (save a CSV listing the test filenames).
- **Test set must be untouched** until the final evaluation (no hyperparameter tuning using test).

---

## ðŸ§  Model Design (baseline CNN)

**Architecture (example, high-level):**

- Conv2D(32, 3Ã—3) â†’ ReLU â†’ MaxPool(2Ã—2)
- Conv2D(64, 3Ã—3) â†’ ReLU â†’ MaxPool(2Ã—2)
- Conv2D(128, 3Ã—3) â†’ ReLU â†’ MaxPool(2Ã—2)
- Flatten â†’ Dense(128) â†’ ReLU â†’ Dropout(0.4)
- Output Dense(num\_classes) â†’ Softmax

**Enhancements to try:**

- Batch Normalization after conv blocks.
- Dropout in dense layer(s) to reduce overfitting.
- Weight decay (L2 regularization) if overfitting persists.
- Transfer learning: MobileNetV2 / EfficientNet/B0 as feature extractor if dataset small.

**Why this baseline?**

- Simple, fast to train, interpretable. Good starting point before trying heavier models.

---

## âš™ï¸ Training Procedure & Hyperparameters

**Loss & optimizer**

- Loss: `categorical_crossentropy` (for multi-class).
- Optimizer: `Adam` (initial lr = 1e-3).

**Training schedule**

- Batch size: 16â€“64 (try 32 as starting point).
- Epochs: 30â€“50 with early stopping (monitor `val_loss` with patience=5).
- Callbacks: EarlyStopping, ModelCheckpoint (save best by `val_loss`), ReduceLROnPlateau.

**Validation**

- Use validation set during training to select best model and tune hyperparameters.

**Metrics to track**

- Accuracy, Precision, Recall, F1-score (per-class), Confusion Matrix.

---

## ðŸ“¦ Model Saving & Formats

**Framework-native formats**

- **Keras/TensorFlow**: `.h5` (HDF5) or SavedModel directory.
- **PyTorch**: `state_dict()` â†’ `.pth`.

**Interchange formats**

- **ONNX**: export if you need cross-framework compatibility.
- **TensorFlow Lite**: for mobile deployment (optional later).

**Important**

- Avoid using `pickle` to save heavy DL models. Use framework serializers.
- Save also the *preprocessing metadata* (image size, normalization scheme) and the label map (class â†’ index).

---

## ðŸ“Š Evaluation & Reporting

- Evaluate the final model only on the untouched **test set**.
- Produce:
  - Test accuracy and loss.
  - Per-class precision/recall/F1.
  - Confusion matrix image.
  - Training curves (loss & accuracy) for train vs val.
- Save evaluation artifacts and log them to MLflow.

---

## ðŸ” MLflow â€” Experiment Tracking (what to log)

**What to log**

- Parameters: architecture name, learning rate, batch size, epochs, augmentation used.
- Metrics: train/val accuracy & loss per epoch, final test metrics.
- Artifacts: saved model file, training plots (png), confusion matrix (png), sample predictions csv.
- Dataset version/hash (or metadata.csv) and git commit id for reproducibility.

**How to use**

- Create an experiment for this project.
- For each run, log params, metrics and artifacts.
- Use `mlflow ui` to compare runs and pick the best configuration.

---

## ðŸ§¾ Deliverables (before submission)

- Well-structured repository with `README.md`.
- `src/` scripts: preprocess, train, evaluate, inference utility.
- `models/` directory with the final model file(s).
- `requirements.txt` (or `environment.yml`).
- MLflow experiment with logged runs and the best run marked.
- Short report (PDF or Markdown) summarizing results and decisions.
- Optional: short demo video or Streamlit app to show inference.

---

## âš™ï¸ Environment & Dependencies (suggestion)

- Python >= 3.8
- TensorFlow or PyTorch (choose one; examples in project assume TensorFlow/Keras)
- OpenCV (`opencv-python`)
- scikit-learn
- pandas, matplotlib
- mlflow

Put exact package versions in `requirements.txt` for reproducibility.

---

## ðŸ”­ Next steps & Improvements (post-submission)

- Move to landmark-based approach (MediaPipe) for lighter-weight real-time inference.
- Support dynamic gestures using temporal models (LSTM/Transformer / 3D-CNN).
- Try transfer learning with MobileNet / EfficientNet for better accuracy with limited data.
- Convert model to TensorFlow Lite for mobile deployment.

---

## ðŸ“Œ Notes & Best Practices

- Keep a single source of truth for preprocessing; inference must apply exactly the same steps.
- Save random seeds and the split lists to avoid accidental leakage.
- Document any manual cleaning or filtering you do on the dataset.
